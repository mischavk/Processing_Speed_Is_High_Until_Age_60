{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from numba import njit\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import binom\n",
    "from functools import partial\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Import our bayesflow lib\n",
    "from deep_bayes.models import BayesFlow, InvariantNetwork\n",
    "from deep_bayes.training import train_online\n",
    "from deep_bayes.losses import maximum_likelihood_loss\n",
    "from deep_bayes.viz import plot_true_est_scatter, plot_true_est_posterior\n",
    "import deep_bayes.diagnostics as diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.__version__.startswith('1'):\n",
    "    tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Network Structure\n",
    "Here, we will define the basic outline of a permutation-invariant neural network which maps raw reaction times data to outcomes.\n",
    "<br>\n",
    "See https://arxiv.org/pdf/1901.06082.pdf (p.28) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     53,
     97
    ]
   },
   "outputs": [],
   "source": [
    "class InvariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an invariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, meta, pooler=tf.reduce_mean):\n",
    "        \"\"\"\n",
    "        Creates an invariant function with mean pooling.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta : dict -- a dictionary with hyperparameter name - values\n",
    "        \"\"\"\n",
    "\n",
    "        super(InvariantModule, self).__init__()\n",
    "\n",
    "\n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_inv_args'])\n",
    "            for _ in range(meta['n_dense_inv'])\n",
    "        ])\n",
    "        \n",
    "        self.pooler = pooler\n",
    "            \n",
    "\n",
    "        self.post_pooling_dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_inv_args'])\n",
    "            for _ in range(meta['n_dense_inv'])\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Transofrms the input into an invariant representation.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or 'samples' dimensions\n",
    "            over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        # Embed\n",
    "        x_emb = self.module(x)\n",
    "\n",
    "        # Pool representation\n",
    "        pooled = self.pooler(x_emb, axis=1)\n",
    "    \n",
    "        # Increase representational power\n",
    "        out = self.post_pooling_dense(pooled)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EquivariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an equivariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, meta):\n",
    "        \"\"\"\n",
    "        Creates an equivariant neural network consisting of a FC network with\n",
    "        equal number of hidden units in each layer and an invariant module\n",
    "        with the same FC structure.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta : dict -- a dictionary with hyperparameter name - values\n",
    "        \"\"\"\n",
    "\n",
    "        super(EquivariantModule, self).__init__()\n",
    "\n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_equiv_args'])\n",
    "            for _ in range(meta['n_dense_equiv'])\n",
    "        ])\n",
    "\n",
    "        self.invariant_module = InvariantModule(meta)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Transofrms the input into an equivariant representation.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or 'samples' dimensions\n",
    "            over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        x_inv = self.invariant_module(x)\n",
    "        x_inv = tf.stack([x_inv] * int(x.shape[1]), axis=1) # Repeat x_inv n times\n",
    "        x = tf.concat((x_inv, x), axis=-1)\n",
    "        out = self.module(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InvariantNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Implements a network which parameterizes a\n",
    "    permutationally invariant function according to Bloem-Reddy and Teh (2019).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, meta):\n",
    "        \"\"\"\n",
    "        Creates a permutationally invariant network\n",
    "        consisting of two equivariant modules and one invariant module.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta : dict -- hyperparameter settings for the equivariant and invariant modules\n",
    "        \"\"\"\n",
    "\n",
    "        super(InvariantNetwork, self).__init__()\n",
    "\n",
    "        self.equiv = tf.keras.Sequential([\n",
    "            EquivariantModule(meta)\n",
    "            for _ in range(meta['n_equiv'])\n",
    "        ])\n",
    "        self.inv = InvariantModule(meta)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Transofrms the input into a permutationally invariant\n",
    "        representation by first passing it through multiple equivariant\n",
    "        modules in order to increase representational power.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the\n",
    "        'samples' dimensions over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.equiv(x)\n",
    "        out = self.inv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter settings and model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network structure\n",
    "summary_meta = {\n",
    "    'dense_inv_args'   :  dict(units=64, activation='elu', kernel_initializer='glorot_normal'),\n",
    "    'dense_equiv_args' :  dict(units=64, activation='elu', kernel_initializer='glorot_normal'),\n",
    "    'dense_post_args'  :  dict(units=64, activation='elu', kernel_initializer='glorot_normal'),\n",
    "    'n_equiv'          :  2,\n",
    "    'n_dense_inv'      :  3,\n",
    "    'n_dense_equiv'    :  3,\n",
    "}\n",
    "\n",
    "# Network hyperparameters\n",
    "inv_meta = {\n",
    "    'n_units': [128, 128, 128],\n",
    "    'activation': 'elu',\n",
    "    'w_decay': 0.00000,\n",
    "    'initializer': 'glorot_uniform'\n",
    "}\n",
    "n_inv_blocks = 4\n",
    "\n",
    "# Forward model hyperparameters\n",
    "param_names = [r'$v_1$', r'$v_2$', r'$a_1$', r'$a_2$', r'$\\tau_{c}$', r'$\\tau_{w}$']\n",
    "theta_dim = len(param_names)\n",
    "n_test = 300\n",
    "n_obs_max = 60\n",
    "n_obs_min = 60\n",
    "n_obs_test = (60, 60)\n",
    "\n",
    "# Utility for online learning\n",
    "data_gen = partial(data_generator, n_obs_min=n_obs_min, n_obs_max=n_obs_max)\n",
    "\n",
    "\n",
    "# Training and optimizer hyperparameters\n",
    "ckpt_file = \"iat_bayesflow\"\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "iterations_per_epoch = 1000\n",
    "n_samples_posterior = 2000\n",
    "clip_value = 5.\n",
    "\n",
    "learning_rate = 0.001\n",
    "if tf.__version__.startswith('1'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_net = InvariantNetwork(summary_meta)\n",
    "model = BayesFlow(inv_meta, n_inv_blocks, theta_dim, summary_net=summary_net, permute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint manager\n",
    "Used for saving/loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ./checkpoints/iat_bayesflow\\ckpt-57\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './checkpoints/iat_bayesflow', max_to_keep=5)\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on all real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# 1. Store all data-set chunk names in a list os.listdir()\n",
    "\n",
    "# 2. For each chunk\n",
    "\n",
    "    # 2.1 Load chunk\n",
    "    \n",
    "    \n",
    "    # 2.2 Preprocess chunk \n",
    "        # 2.2.1 Apply IAT exclusion criteria (conservative)\n",
    "        # 2.2.2 Format data for NN, negative coding\n",
    "        # 2.2.3 Add 0s for <0.3, and >10\n",
    "        \n",
    "        \n",
    "    # 2.3 Estimate chunk\n",
    "        # theta_samples = np.concatenate([model.sample(x, n_post_samples_sbc, to_numpy=True) \n",
    "        # for x in tf.split(sbc_data['x'].astype(np.float32), 10, axis=0)], axis=1)\n",
    "        \n",
    "        \n",
    "    # 2.5 Compute summaries: means, medians, stds, Q0.025, Q0.0975, post_corr\n",
    "        \n",
    "        \n",
    "    # 2.4 Post-processing \n",
    "        # 2.4.1 find inices of implausible (out of prior) parameter means\n",
    "        # 2.4.2 remove from estimates\n",
    "        # 2.4.3 remove from datasets\n",
    "    \n",
    "    \n",
    "    # 2.6 Store everything together (serialized, pickle.dump) as a dict with keys \n",
    "        # dict_to_store = {'data_array': data_chunk_clean, 'est_array': estimates_clean}\n",
    "        \n",
    "# 3. Celebrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get X_test into the correct format:\n",
    "\n",
    "# rts = np.where(X_test[:, :, 1], -X_test[:, :, 0], X_test[:, :, 0])\n",
    "# comps = X_test[:, :, 2]\n",
    "# X_test = np.stack((rts, comps), axis=2)\n",
    "\n",
    "# # Exclusion criterion (< 0.3)\n",
    "# idx_300 = (np.abs(X_test[:, :, 0]) < 0.3).sum(axis=1) <= 12\n",
    "# X_test = X_test[idx_300, :, :]\n",
    "\n",
    "# # Exclusion criterion (> 10)\n",
    "# idx_10000 = (np.abs(X_test)[:, :, 0] > 10.0).sum(axis=1) == 0\n",
    "# X_test = X_test[idx_10000, :, :]\n",
    "\n",
    "# # Keep only corresponding y\n",
    "# y_test = y_test[idx_300, :]\n",
    "# y_test = y_test[idx_10000, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "175.733px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
